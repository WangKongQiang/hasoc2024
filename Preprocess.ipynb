{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-oXjA06e4iVq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import string\r\n",
    "import re\r\n",
    "\r\n",
    "import nltk\r\n",
    "from nltk import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3-Y-d2TF44s2"
   },
   "outputs": [],
   "source": [
    "def remove_punch(text):\r\n",
    "    x = str(text)\r\n",
    "    for p in string.punctuation:\r\n",
    "        x = x.replace(p,'')\r\n",
    "    return x\r\n",
    "\r\n",
    "def deEmojify(text):\r\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\r\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "                           \"]+\", flags = re.UNICODE)\r\n",
    "    return regrex_pattern.sub(r'',text)\r\n",
    "\r\n",
    "def lower_tokens(tokens):\r\n",
    "    return  [w.lower() for w in tokens]\r\n",
    "\r\n",
    "def remove_stopwords(tokens):\r\n",
    "    stop = set(stopwords.words(\"english\"))\r\n",
    "    filtered_words = [word for word in tokens if word not in stop]\r\n",
    "    return \" \".join(filtered_words)\r\n",
    "\r\n",
    "def remove_names(text):\r\n",
    "    for word in text.split():\r\n",
    "        if word[0] == \"@\":\r\n",
    "            text = text.replace(word, \"\")\r\n",
    "    return text\r\n",
    "\r\n",
    "def remove_url(text):\r\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\r\n",
    "    result = re.sub(r\"https\\S+\", \"\", text)\r\n",
    "    return result\r\n",
    "\r\n",
    "\r\n",
    "def decode_unicode(text):\r\n",
    "    text = text.decode('utf-8')\r\n",
    "    return text\r\n",
    "\r\n",
    "\r\n",
    "def text_to_wordlist(text,stem_words=True):\r\n",
    "    # Clean the text\r\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\r\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\r\n",
    "    text = re.sub(r\"doesn't\", \"does not \", text)\r\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\r\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\r\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\r\n",
    "    text = re.sub(r\"n't\", \" not \", text)\r\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\r\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\r\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\r\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\r\n",
    "    text = re.sub(r\",\", \" \", text)\r\n",
    "    text = re.sub(r\"\\.\", \" \", text)\r\n",
    "    text = re.sub(r\"!\", \" ! \", text)\r\n",
    "    text = re.sub(r\"\\/\", \" \", text)\r\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\r\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\r\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\r\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\r\n",
    "    text = re.sub(r\"'\", \" \", text)\r\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\r\n",
    "    text = re.sub(r\":\", \" : \", text)\r\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\r\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\r\n",
    "    text = re.sub(r\" u s \", \" american \", text)\r\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\r\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\r\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\r\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\r\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\r\n",
    "    \r\n",
    "    # Optionally, shorten words to their stems\r\n",
    "    if stem_words:\r\n",
    "        text = text.split()\r\n",
    "        stemmer = SnowballStemmer('english')\r\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\r\n",
    "        text = \" \".join(stemmed_words)\r\n",
    "    \r\n",
    "    # remove consecutive letters to single letter at end\r\n",
    "    text = re.sub(r'(.)\\1+$', r'\\1', text)\r\n",
    "    \r\n",
    "    # Return a list of words\r\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHMxtpLx6k67",
    "outputId": "cbc14edf-a2cf-4c10-c60f-8a8691b78eba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\8888\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-4CX0GAt479y"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('en_Hasoc2021_train.csv')\n",
    "df = pd.read_csv('en_Hasoc2021_test_task1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QZkP-mqM-6Bw"
   },
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text'].apply(lambda text:remove_names(text))\r\n",
    "df['text_clean'] = df['text_clean'].apply(lambda text:remove_url(text))\r\n",
    "df['text_clean'] = df['text_clean'].apply(lambda text:text_to_wordlist(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "xG-OvUSl6KW2"
   },
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_clean'].apply(lambda text:deEmojify(text))\r\n",
    "df['text_clean'] = df['text_clean'].apply(lambda text:remove_punch(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\8888\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Dxfcsz6C7bjv"
   },
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(tweet) for tweet in df['text_clean']]\r\n",
    "lower = [lower_tokens(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apyiK_Gy70om",
    "outputId": "73ea0944-f22b-45d3-b582-bb66051d0299"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\8888\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fws_JgDL8ZE4"
   },
   "outputs": [],
   "source": [
    "removed = []\r\n",
    "for tokens in lower:\r\n",
    "    removed.append(remove_stopwords(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = pd.Series(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "DE3b38nS8r0W",
    "outputId": "a26b24b0-dca9-449d-c771-9951a207a51c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60c5d6bf5659ea5e55deffcb</td>\n",
       "      <td>Fewer people coming in for vaccinations. So sa...</td>\n",
       "      <td>fewer peopl come vaccin sad nurs covidvaccin vumc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60c5d6bf5659ea5e55df028c</td>\n",
       "      <td>@MattHancock This may all be true. But... What...</td>\n",
       "      <td>may true piss big dom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60c5d6bf5659ea5e55def377</td>\n",
       "      <td>@Layla_EFC Iâ€™ve unfollowed him the wanker</td>\n",
       "      <td>unfollow wanker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60c5d6bf5659ea5e55def4c7</td>\n",
       "      <td>You guys are losing it all over the world. The...</td>\n",
       "      <td>guy lose world jung label islamophobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60c5d6bf5659ea5e55df01a6</td>\n",
       "      <td>And thus death laughs... It is sad merriment, ...</td>\n",
       "      <td>thus death laugh sad merriment still covid ind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  60c5d6bf5659ea5e55deffcb   \n",
       "1  60c5d6bf5659ea5e55df028c   \n",
       "2  60c5d6bf5659ea5e55def377   \n",
       "3  60c5d6bf5659ea5e55def4c7   \n",
       "4  60c5d6bf5659ea5e55df01a6   \n",
       "\n",
       "                                                text  \\\n",
       "0  Fewer people coming in for vaccinations. So sa...   \n",
       "1  @MattHancock This may all be true. But... What...   \n",
       "2          @Layla_EFC Iâ€™ve unfollowed him the wanker   \n",
       "3  You guys are losing it all over the world. The...   \n",
       "4  And thus death laughs... It is sad merriment, ...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  fewer peopl come vaccin sad nurs covidvaccin vumc  \n",
       "1                              may true piss big dom  \n",
       "2                                    unfollow wanker  \n",
       "3             guy lose world jung label islamophobia  \n",
       "4  thus death laugh sad merriment still covid ind...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VhyWQWtGAK4M"
   },
   "outputs": [],
   "source": [
    "# df.to_csv('preprocess_data_analysis.csv')\n",
    "df.to_csv('preprocess_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Preprocess.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "95c10dbc6f7eccef0c1ace84822d618f7863d3bc26cab307fc0169bb43c23fbe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
